import re
import types
import os
import stat
import pprint

from integralstor import datetime_utils, disks, ramdisk, filesize, command, config, db

"""
Basic pool information parsing functions
"""


def get_single_line_value(lines, property_name):
    """Given a property in the form of 'property_name: property_value' return the value of the property."""
    retval = None
    try:
        if not lines:
            raise Exception("No lines provided!")
        if '%s:' % property_name not in lines[0]:
            raise Exception("%s not found in the line provided!" %
                            property_name)
        parts = lines[0].split()
        if not parts or len(parts) < 2:
            raise Exception("%s not found in the line provided!" %
                            property_name)
        retval = parts[1]
    except Exception, e:
        return None, 'Error getting the single line value : %s' % str(e)
    else:
        return retval, None


def get_multi_line_value(lines, property_name):
    """Given a property in the form of 'property_name: property_value' followed by more lines in the list it returns the value of the property in the form of a string consisting of all following lines in the list."""
    retval = None
    try:
        # if property_name == 'status':
        # print lines
        if not lines or '%s:' % property_name not in lines[0]:
            raise Exception("%s not found in the line provided!" %
                            property_name)
        str = ""
        for line in lines:
            res = re.match('^%s:\s*([\s\S]+)' % property_name, line.strip())
            if res:
                str += res.groups()[0]
            else:
                str += line.strip()
            str += ' '
        retval = str
    except Exception, e:
        return None, 'Error getting the multi line value : %s' % str(e)
    else:
        return retval, None


def get_config_line_details(line):
    """Given a pool component configuration line, return a dict with each of its components."""
    d = None
    try:
        parts = line.split()
        if not parts:
            return None, None
        d = {}
        d['name'] = parts[0]
        if 'raidz1' in d['name']:
            d['type'] = 'raid5'
        elif 'logs' in d['name']:
            d['type'] = 'logs'
        elif 'cache' in d['name']:
            d['type'] = 'cache'
        elif 'raidz2' in d['name']:
            d['type'] = 'raid6'
        elif 'mirror' in d['name']:
            d['type'] = 'mirror'
        elif 'ramdisk' in d['name'].lower():
            d['type'] = 'RAMDisk'
            rds, err = ramdisk.get_ramdisks_config()
            if rds:
                for rd in rds:
                    if rd['path'].lower() in d['name'].lower():
                        d['ramdisk_size'] = rd['size']
                        break
        else:
            d['type'] = 'disk'

        if d['type'] not in ['cache', 'logs']:
            status_d = {}
            status_d['state'] = parts[1]
            status_d['read'] = parts[2]
            status_d['write'] = parts[3]
            status_d['chksum'] = parts[4]
            d['status'] = status_d
    except Exception, e:
        return None, 'Error getting config line details : %s' % str(e)
    else:
        return d, None


def process_config_section(lines):
    """Given a list of lines for a config section, return a dict of each of its components along with their children consitituents and the root of the tree."""
    root_node = None
    root_name = None
    try:
        stack = []
        if not lines:
            return None, None
        prev_node = None
        curr_spaces = -1
        d = None

        for line in lines:
            if d:
                prev_node = d
            d, err = get_config_line_details(line)
            if not d:
                errstr = "Error getting config line details for line : %s." % line
                if err:
                    errstr += "Error : %s" % err
                raise Exception(errstr)
            prev_spaces = curr_spaces
            curr_spaces = len(line.rstrip()) - len(line.rstrip().lstrip())
            if curr_spaces > prev_spaces:
                if prev_node:
                    stack.append(prev_node)
            elif curr_spaces < prev_spaces:
                stack.pop()
            if stack:
                parent = stack[len(stack) - 1]
                d['parent'] = parent['name']
                if "children" not in parent:
                    parent['children'] = []
                # parent['children'].append(d['name'])
                parent['children'].append(d)
            else:
                d['parent'] = None
                root_name = d['name']
                root_node = d
                #nodes[d['name']] = d

        if root_node['name'] not in ['cache', 'logs']:
            root_node['type'] = 'pool'

        #pp = pprint.PrettyPrinter(indent=4)
        #pp.pprint( nodes)
    except Exception, e:
        return None, None, 'Error processing the configuration section : %s' % str(e)
    else:
        return root_node, root_name, None


def process_spares_section(lines):
    spares_dict = {}
    try:
        for line in lines:
            if line.strip() == 'spares':
                continue
            # print line
            parts = line.split()
            if not parts:
                raise Exception('Invalid spares section detected')
            spares_dict[parts[0]] = parts[1]
    except Exception, e:
        return None, 'Error processing the spares configuration section : %s' % str(e)
    else:
        return spares_dict, None


def process_pool_config(lines):
    """Process the complete config section of the zpool status command and return a dict for each of pool, cache and logs sections."""
    return_dict = {}
    try:
        if not lines:
            raise Exception("No lines passed!")
        if 'config:' not in lines[0]:
            raise Exception("No config section found!")
        start_processing = False
        base_space_count = -1
        component_lines = []
        processing = None
        d = {}
        for line in lines:
            res = re.match(
                '^NAME[\s]*STATE[\s]*READ[\s]*WRITE[\s]*CKSUM', line.strip())
            if res:
                start_processing = True
                base_space_count = len(line.rstrip()) - \
                    len(line.rstrip().lstrip())
                # print base_space_count
                continue
            elif not start_processing:
                continue
            space_count = len(line.rstrip()) - len(line.rstrip().lstrip())
            if space_count == base_space_count:
                # Has to be a pool line, logs line or cache line
                if component_lines:
                    if processing == 'cache':
                        d['cache'] = component_lines
                    elif processing == 'logs':
                        d['logs'] = component_lines
                    elif processing == 'spares':
                        d['spares'] = component_lines
                    else:
                        d['pool'] = component_lines
                    component_lines = []
                if line.strip() == 'cache':
                    processing = 'cache'
                elif line.strip() == 'logs':
                    processing = 'logs'
                elif line.strip() == 'spares':
                    processing = 'spares'
                else:
                    processing = 'pool'
            component_lines.append(line)
        if component_lines:
            d[processing] = component_lines

        # We have now split the lines up into sections so process each section

        if 'logs' in d:
            root_node, root_name, err = process_config_section(d['logs'])
            if not root_node:
                errstr = "Error retrieving logs config section"
                if err:
                    errstr += err
                raise Exception(errstr)
            return_dict['logs'] = {}
            return_dict['logs']['root'] = root_node
            # print return_dict['logs']
            vdev_type, err = get_vdev_type(return_dict['logs'])
            # print '1', vdev_type, err
            if not vdev_type:
                if err:
                    raise Exception(err)
                else:
                    raise Exception('Error retrieving vdev type')
            return_dict['logs']['type'] = vdev_type
        else:
            return_dict['logs'] = None

        if 'cache' in d:
            root_node, root_name, err = process_config_section(d['cache'])
            if not root_node:
                errstr = "Error retrieving cache config section"
                if err:
                    errstr += err
                raise Exception(errstr)
            return_dict['cache'] = {}
            return_dict['cache']['root'] = root_node
            vdev_type, err = get_vdev_type(return_dict['cache'])
            # print '2', vdev_type, err
            if not vdev_type:
                if err:
                    raise Exception(err)
                else:
                    raise Exception('Error retrieving vdev type')
            return_dict['cache']['type'] = vdev_type
        else:
            return_dict['cache'] = None

        if 'spares' in d:
            spares_dict, err = process_spares_section(d['spares'])
            if err:
                raise Exception(err)
            return_dict['spares'] = spares_dict
        else:
            return_dict['spares'] = None

        root_node, root_name, err = process_config_section(d['pool'])
        if not root_node:
            errstr = "Error retrieving pool config section"
            if err:
                errstr += err
            raise Exception(errstr)
        return_dict['pool'] = {}
        return_dict['pool']['root'] = root_node
        vdev_type, err = get_vdev_type(return_dict['pool'])
        # print '3', vdev_type, err
        if not vdev_type:
            if err:
                raise Exception(err)
            else:
                raise Exception('Error retrieving vdev type')
        return_dict['pool']['type'] = vdev_type

        #pp = pprint.PrettyPrinter(indent=4)
        # pp.pprint(return_dict)

    except Exception, e:
        return None, "Error processing the pool's config section : %s" % str(e)
    else:
        return return_dict, None


def get_vdev_type(d):
    ''' 
    Given the dictionary within config of either the pool, cache or logs, return the type of VDEV.
    Only striped(raid0), mirror(raid1), raidz1(raid5), raidz2(raid6), striped-mirror(raid10) are currently supported.
    '''
    vdev_type = None
    try:
        if not d:
            raise Exception('Config section null.')
        if 'root' not in d or not d['root']:
            raise Exception('Root component in vdev is null.')
        if 'children' not in d['root']:
            raise Exception('No child components in root vdev .')

        kids = d['root']['children']

        if not kids:
            raise Exception('Child component in root vdev is null.')

        if len(kids) > 1:
            # Either raid0 or raid 10
            if 'mirror-' in kids[0]['name']:
                vdev_type = 'raid10'
            elif 'raidz1-' in kids[0]['name']:
                vdev_type = 'raid50'
            elif 'raidz2-' in kids[0]['name']:
                vdev_type = 'raid60'
            else:
                vdev_type = 'raid0'
        else:
            # Either raid1, raid5 or raid6
            if 'mirror-' in kids[0]['name']:
                vdev_type = 'raid1'
            elif 'raidz1-' in kids[0]['name']:
                vdev_type = 'raid5'
            elif 'raidz2-' in kids[0]['name']:
                vdev_type = 'raid6'
            else:
                vdev_type = 'raid0'
    except Exception, e:
        return None, "Error determining the vdev type : %s" % str(e)
    else:
        return vdev_type, None


def process_pool(lines):
    """Given a list of lines corresponding to one pool, process it and return a dict with all its info."""
    return_dict = {}
    try:
        processing = None
        tmp_list = []
        dict = {}
        for line in lines:
            if 'state:' in line.strip():
                if processing and tmp_list:
                    dict[processing] = tmp_list
                tmp_list = []
                processing = 'state'
                # print 'Processing state'
            elif 'status:' in line.strip():
                if processing and tmp_list:
                    dict[processing] = tmp_list
                tmp_list = []
                processing = 'status'
                # print 'Processing status'
            elif 'pool:' in line.strip():
                if processing and tmp_list:
                    dict[processing] = tmp_list
                tmp_list = []
                processing = 'pool'
                # print 'Processing status'
            elif 'scan:' in line.strip():
                if processing and tmp_list:
                    dict[processing] = tmp_list
                tmp_list = []
                processing = 'scan'
            # print 'Processing scan'
            elif 'action:' in line.strip():
                if processing and tmp_list:
                    dict[processing] = tmp_list
                tmp_list = []
                processing = 'action'
                # print 'Processing action'
            elif 'see:' in line.strip():
                if processing and tmp_list:
                    dict[processing] = tmp_list
                tmp_list = []
                processing = 'see'
                # print 'Processing see'
            elif 'scrub:' in line.strip():
                if processing and tmp_list:
                    dict[processing] = tmp_list
                tmp_list = []
                processing = 'scrub'
                # print 'Processing scrub'
            elif 'errors:' in line.strip():
                if processing and tmp_list:
                    dict[processing] = tmp_list
                tmp_list = []
                processing = 'errors'
                # print 'Processing errors'
            elif 'config:' in line.strip():
                if processing and tmp_list:
                    dict[processing] = tmp_list
                tmp_list = []
                processing = 'config'
                # print 'Processing config'
            # print line
            tmp_list.append(line)
        if processing and tmp_list:
            dict[processing] = tmp_list

        # print 'pool name is ' + get_single_line_value(dict['pool'], 'pool')
        # print 'state is ' + get_single_line_value(dict['state'], 'state')
        # print 'errors is ' + get_multi_line_value(dict['errors'], 'errors')
        # print 'scan is ' + get_multi_line_value(dict['scan'], 'scan')

        if 'pool' in dict:
            temp, err = get_single_line_value(dict['pool'], 'pool')
            if not temp:
                errstr = 'Error getting pool name.'
                if err:
                    errstr += errstr
                raise Exception(errstr)
            return_dict['pool_name'] = temp

        if 'state' in dict:
            temp, err = get_single_line_value(dict['state'], 'state')
            if not temp:
                errstr = 'Error getting pool state.'
                if err:
                    errstr += errstr
                raise Exception(errstr)
            return_dict['state'] = temp

        if 'errors' in dict:
            temp, err = get_multi_line_value(dict['errors'], 'errors')
            if not temp:
                errstr = 'Error getting pool errors.'
                if err:
                    errstr += err
                raise Exception(errstr)
            return_dict['errors'] = temp

        if 'scan' in dict:
            temp, err = get_multi_line_value(dict['scan'], 'scan')
            if not temp:
                errstr = 'Error getting pool scan results.'
                if err:
                    errstr += err
                raise Exception(errstr)
            return_dict['scan'] = temp

        if 'status' in dict:
            temp, err = get_multi_line_value(dict['status'], 'status')
            if not temp:
                errstr = 'Error getting pool status results.'
                if err:
                    errstr += err
                raise Exception(errstr)
            return_dict['status'] = temp

        if 'see' in dict:
            temp, err = get_multi_line_value(dict['see'], 'see')
            if not temp:
                errstr = 'Error getting pool see results.'
                if err:
                    errstr += err
                raise Exception(errstr)
            return_dict['see'] = temp

        if 'config' in dict:
            temp, err = process_pool_config(dict['config'])
            if not temp:
                errstr = 'Error getting pool configuration.'
                if err:
                    errstr += err
                raise Exception(errstr)
            return_dict['config'] = temp

        #pp = pprint.PrettyPrinter(indent=4)
        # pp.pprint(return_dict)

    except Exception, e:
        return None, 'Error processing pool configuration : %s' % str(e)
    else:
        return return_dict, None


def get_disks_in_component(component):
    disks = []
    try:
        if 'children' in component and component['children']:
            for kid in component['children']:
                kid_disks, err = get_disks_in_component(kid)
                if err:
                    raise Exception('Error getting disks in use : %s' % err)
                if kid_disks:
                    disks.extend(kid_disks)
        if component['type'] == 'disk':
            disks.append(component['name'])
    except Exception, e:
        return None, "Error getting disks in use : %s" % str(e)
    else:
        return disks, None


"""
Snapshot-ing related functions
"""


def get_snapshots(name=None):
    snapshots = []
    try:
        if name:
            cmd = 'zfs list -t snapshot -r %s -S name -H' % name
        else:
            # Return all snapshots
            cmd = '/sbin/zfs list  -t snapshot -S name -H'
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)
        if lines:
            for line in lines:
                if line.strip() == 'no datasets available':
                    break
                td, err = _get_ds_compononents(line)
                if td:
                    tmp_list = td['name'].split('@')
                    if tmp_list:
                        td['dataset'] = tmp_list[0]
                        if len(tmp_list) > 1:
                            td['snapshot_name'] = tmp_list[1]
                        snapshots.append(td)
                else:
                    if err:
                        raise Exception(err)
        for snap in snapshots:
            prop, err = get_properties(snap['name'])
            if prop:
                snap['properties'] = prop
    except Exception, e:
        return None, 'Error retrieving snapshots : %s' % str(e)
    else:
        return snapshots, None


def get_latest_snapshot(name=None, zfs_auto_snapshot=True):
    snapshot_name = None
    try:
        cmd = None
        if name:
            if zfs_auto_snapshot:
                cmd = "/sbin/zfs list -r -t snapshot %s -o name,creation -s creation | grep %s@zfs | tail -1" % (
                    name, name)
            else:
                cmd = "/sbin/zfs list -r -t snapshot %s -o name,creation -s creation | tail -1" % (
                    name)
        else:
            # Get latest snapshots across all pools/datasets
            cmd = "/sbin/zfs list -t snapshot -o name,creation -s creation | tail -1"
        line, err = command.get_command_output(cmd, shell=True)
        if err:
            raise Exception(err)
        td, err = _get_ds_compononents(line[0])
        if err:
            raise Exception(err)
        snapshot_name = td['name']
    except Exception, e:
        return None, 'Error getting latest snapshot : %s' % str(e)
    else:
        return snapshot_name, None


def create_snapshot(target, name):
    try:
        if (not target) or (not name):
            raise Exception('Snapshot target or name not specified')
        cmd = '/sbin/zfs snapshot %s@%s' % (target, name)
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)
    except Exception, e:
        return False, 'Error creating snapshot : %s ' % str(e)
    else:
        return True, None


def rename_snapshot(ds_name, snapshot_name, new_snapshot_name):
    try:
        if (not ds_name) or (not snapshot_name) or (not new_snapshot_name):
            raise Exception('Snapshot target, name or new name not specified')

        cmd = '/sbin/zfs rename %s@%s %s@%s' % (
            ds_name, snapshot_name, ds_name, new_snapshot_name)
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)
    except Exception, e:
        return False, 'Error renaming snapshot : %s ' % str(e)
    else:
        return True, None


def delete_snapshot(name):
    try:
        if not name:
            raise Exception('Snapshot name not specified')

        cmd = '/sbin/zfs destroy %s' % name
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)
    except Exception, e:
        return False, 'Error deleting snapshot : %s ' % str(e)
    else:
        return True, None


def delete_all_snapshots(dataset_name):
    """Delete all snapshots of a dataset. Does not force destroy.

    args:       dataset_name - data set name
    returns:    (False/True, None/"Error String")

                - (True, None) if deleted successfully, else
                - (False, "Error string")
    """
    try:
        if not dataset_name:
            raise Exception('Dataset name not specified')
        all_datasets, err = get_all_datasets_and_pools(
            dataset_type='filesystem')
        if err:
            raise Exception(err)
        if not all_datasets or dataset_name not in all_datasets:
            raise Exception("Dataset unavailable")

        cmd = 'zfs list -r %s -H -t snapshot -S name -o name | xargs -r -n 1 zfs destroy' % dataset_name
        lines, err = command.get_command_output(cmd, shell=True)
        if err:
            raise Exception(err)

    except Exception, e:
        return False, 'Error deleting snapshots: %s ' % str(e)
    else:
        return True, None


def rollback_snapshot(name):
    try:
        if not name:
            raise Exception('Snapshot name not specified')

        cmd = '/sbin/zfs rollback %s' % name
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)
    except Exception, e:
        return False, 'Error rolling back to snapshot : %s ' % str(e)
    else:
        return True, None


def schedule_snapshot(target, frequent=None, hourly=None, daily=None, weekly=None, monthly=None):
    try:
        cmd = '/sbin/zfs set com.sun:auto-snapshot=false %s' % target
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)
        cmd = '/sbin/zfs set com.sun:auto-snapshot=false %s' % target
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)

        if frequent:
            cmd = '/sbin/zfs set com.sun:auto-snapshot:frequent=true %s' % target
        else:
            cmd = '/sbin/zfs set com.sun:auto-snapshot:frequent=false %s' % target
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)

        if hourly:
            cmd = '/sbin/zfs set com.sun:auto-snapshot:hourly=true %s' % target
        else:
            cmd = '/sbin/zfs set com.sun:auto-snapshot:hourly=false %s' % target
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)

        if daily:
            cmd = '/sbin/zfs set com.sun:auto-snapshot:daily=true %s' % target
        else:
            cmd = '/sbin/zfs set com.sun:auto-snapshot:daily=false %s' % target
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)

        if weekly:
            cmd = '/sbin/zfs set com.sun:auto-snapshot:weekly=true %s' % target
        else:
            cmd = '/sbin/zfs set com.sun:auto-snapshot:weekly=false %s' % target
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)

        if monthly:
            cmd = '/sbin/zfs set com.sun:auto-snapshot:monthly=true %s' % target
        else:
            cmd = '/sbin/zfs set com.sun:auto-snapshot:monthly=false %s' % target
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)

    except Exception, e:
        return False, 'Error changing snapshot schedule : %s ' % str(e)
    else:
        return True, None


def get_snapshot_schedule(target):
    d = {}
    try:
        r, err = get_property(target, 'com.sun:auto-snapshot')
        if err:
            raise Exception(err)
        if 'value' in r and r['value'] == 'true':
            d['scheduled'] = True
        else:
            d['scheduled'] = False

        r, err = get_property(target, 'com.sun:auto-snapshot:frequent')
        if err:
            raise Exception(err)
        if 'value' in r and r['value'] == 'true':
            d['frequent'] = True
        else:
            d['frequent'] = False

        r, err = get_property(target, 'com.sun:auto-snapshot:hourly')
        if err:
            raise Exception(err)
        if 'value' in r and r['value'] == 'true':
            d['hourly'] = True
        else:
            d['hourly'] = False

        r, err = get_property(target, 'com.sun:auto-snapshot:daily')
        if err:
            raise Exception(err)
        if 'value' in r and r['value'] == 'true':
            d['daily'] = True
        else:
            d['daily'] = False

        r, err = get_property(target, 'com.sun:auto-snapshot:weekly')
        if err:
            raise Exception(err)
        if 'value' in r and r['value'] == 'true':
            d['weekly'] = True
        else:
            d['weekly'] = False

        r, err = get_property(target, 'com.sun:auto-snapshot:monthly')
        if err:
            raise Exception(err)
        if 'value' in r and r['value'] == 'true':
            d['monthly'] = True
        else:
            d['monthly'] = False

    except Exception, e:
        return None, 'Error getting snapshot schedule : %s ' % str(e)
    else:
        return d, None


def get_all_snapshot_schedules():
    schedule = {}
    try:
        lines, err = command.get_command_output('zfs get all')
        if err:
            raise Exception(err)
        auto_snapshots_exist = False
        for line in lines:
            if 'auto-snapshot' in line:
                auto_snapshots_exist = True
                break
        if auto_snapshots_exist:
            lines, err = command.get_command_output(
                'zfs get all | grep auto-snapshot', shell=True)
            if err:
                raise Exception(err)
            if lines:
                for line in lines:
                    parts = line.split()
                    if parts and len(parts) == 4:
                        if parts[2] == 'true':
                            if parts[0] not in schedule:
                                schedule[parts[0]] = {}
                            if parts[1] == 'com.sun:auto-snapshot':
                                schedule[parts[0]]['scheduled'] = True
                            elif 'daily' in parts[1]:
                                schedule[parts[0]]['daily'] = True
                            elif 'hourly' in parts[1]:
                                schedule[parts[0]]['hourly'] = True
                            elif 'weekly' in parts[1]:
                                schedule[parts[0]]['weekly'] = True
                            elif 'monthly' in parts[1]:
                                schedule[parts[0]]['monthly'] = True
                            elif 'frequent' in parts[1]:
                                schedule[parts[0]]['frequent'] = True
    except Exception, e:
        return None, 'Error getting all snapshot schedules : %s ' % str(e)
    else:
        return schedule, None


"""
Dataset/pool properties related functions
"""


def _get_property_compononents(line, property_index, value_index, source_index):
    d = None
    try:
        if not line:
            raise Exception(
                'Error getting property component : No line specified')
        d = {}
        d['name'] = line[property_index:value_index].strip()
        d['value'] = line[value_index:source_index].strip()
        d['source'] = line[source_index:].strip()
    except Exception, e:
        return None, str(e)
    else:
        return d, None


def get_properties(name, pool_properties=False):
    properties = {}
    try:
        if pool_properties:
            cmd = '/sbin/zpool get all %s ' % name
        else:
            cmd = '/sbin/zfs get all %s ' % name
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)

        if lines:
            property_index = lines[0].find('PROPERTY')
            value_index = lines[0].find('VALUE')
            source_index = lines[0].find('SOURCE')

            for line in lines[1:]:
                td, err = _get_property_compononents(
                    line, property_index, value_index, source_index)
                if td:
                    properties[td['name']] = td
                else:
                    if err:
                        raise Exception(err)
    except Exception, e:
        return None, 'Error retrieving properties : %s' % str(e)
    else:
        return properties, None


def get_property(target, prop_name):
    d = {}
    try:
        cmd = '/sbin/zfs get %s %s' % (prop_name, target)
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)

        if lines and len(lines) == 2:
            property_index = lines[0].find('PROPERTY')
            value_index = lines[0].find('VALUE')
            source_index = lines[0].find('SOURCE')

            d, err = _get_property_compononents(
                lines[1], property_index, value_index, source_index)
            if err:
                raise Exception(err)
    except Exception, e:
        if prop_name:
            return None, 'Error retrieving property %s : %s' % (prop_name, str(e))
        else:
            return None, 'Error retrieving property  : %s' % str(e)
    else:
        return d, None


def update_dataset_zvol_property(name, prop_name, prop_value):
    try:
        if (not name) or (not prop_name) or (not prop_value):
            raise Exception('Required parameters not passed')

        cmd = '/sbin/zfs set %s=%s %s' % (prop_name, prop_value, name)
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)
    except Exception, e:
        if prop_name:
            return False, 'Error saving property %s : %s ' % (prop_name, str(e))
        else:
            return False, 'Error saving property  : %s ' % str(e)
    else:
        return True, None


def get_exposed_ds_zvol_properties(properties):
    return_dict = {}
    try:
        is_zvol = True
        if properties['type']['value'] == 'filesystem':
            is_zvol = False

        exposed_ds_zvol_properties = {'compression': {'short_desc': 'Compression', 'long_desc': 'Controls the compression algorithm used for this dataset. Setting compression to on indicates that the current default compression algorithm should be used.  The default balances compression and decompression  speed, with compression ratio and is expected to work well on a wide variety of workloads.'},
                                      'compressratio': {'short_desc': 'Compression ratio', 'long_desc': 'Compression ration'},
                                      'dedup': {'short_desc': 'Deduplication', 'long_desc': 'Controls whether deduplication is in effect for a dataset.'},
                                      'type': {'short_desc': 'Type', 'long_desc': 'Type'},
                                      'usedbychildren': {'short_desc': 'Space used by children', 'long_desc': 'The amount of space used by children of this dataset, which would be freed if all the datasets children were destroyed.'},
                                      'usedbydataset': {'short_desc': 'Space used by this dataset', 'long_desc': 'The amount of space used by this dataset itself, which would be freed if the dataset were destroyed (after first  removing  any  refreservation and destroying any necessary snapshots or descendents).'},
                                      'creation': {'short_desc': 'Created on', 'long_desc': 'Created on'},
                                      'logicalreferenced': {'short_desc': 'Logical referenced', 'long_desc': 'The  amount of space that is "logically" accessible by this dataset.'},
                                      'logicalused': {'short_desc': 'Logical used', 'long_desc': 'The  amount  of  space that is "logically" consumed by this dataset and all its descendents.'},
                                      'usedbysnapshots': {'short_desc': 'Space used by snapshots', 'long_desc': 'The  amount  of  space  consumed  by  snapshots  of  this  dataset.'},
                                      'usedbydataset': {'short_desc': 'Space used by dataset', 'long_desc': 'The amount of space used by this dataset itself, which would be freed if the dataset were destroyed.'},
                                      'written': {'short_desc': 'Written to dataset', 'long_desc': 'The amount of referenced space written to this dataset since the previous snapshot.'},
                                      'aclinherit': {'short_desc': 'ACL inheritance', 'long_desc': 'Controls how ACL entries are inherited when files and directories are created.'},
                                      'acltype': {'short_desc': 'ACL type', 'long_desc': 'Controls whether ACLs are enabled and if so what type of ACL to use.'},
                                      'atime': {'short_desc': 'Access time on reads', 'long_desc': 'Controls whether the access time for files is updated when they are read.'},
                                      'checksum': {'short_desc': 'Checksum algorithm', 'long_desc': 'Controls the checksum used to verify data integrity.'},
                                      'copies': {'short_desc': 'Copies', 'long_desc': 'Controls  the  number of copies of data stored for this dataset. These copies are in addition to any redundancy provided by the pool, for example, mirroring or RAID-Z. The copies are stored on different disks, if possible. The space used by multiple copies is charged to the associated file and dataset, changing the used property and counting against quotas and reservations. Changing  this  property  only  affects  newly-written data.'},
                                      'devices': {'short_desc': 'Device nodes', 'long_desc': 'Controls whether device nodes can be opened on this file system.'},
                                      'exec': {'short_desc': 'Process execution', 'long_desc': 'Controls whether processes can be executed from within this file system.'},
                                      'primarycache': {'short_desc': 'Primary cache', 'long_desc': 'Controls  what  is cached in the primary cache (ARC).'},
                                      'snapshot_limit': {'short_desc': 'Snapshot limit', 'long_desc': ' Limits the number of snapshots that can be created on a dataset and its descendents.'},
                                      'readonly': {'short_desc': 'Read only', 'long_desc': 'Controls whether this dataset can be modified.'},
                                      'redundant_metadata': {'short_desc': 'Redundant metadata', 'long_desc': 'Controls  what  types  of  metadata  are stored redundantly.  ZFS stores an extra copy of metadata, so that if a single block is corrupted, the amount of user data lost is limited.  This extra copy is in addition to any redundancy provided at the pool level (e.g. by mirroring  or  RAID-Z),  and  is  in  addition to an extra copy specified by the copies property (up to a total of 3 copies). When set to all, ZFS stores an extra copy of all metadata.  If a single on-disk block is corrupt, at worst a single block of user  data  (which is recordsize bytes long) can be lost.  When  set  to  most,  ZFS stores an extra copy of most types of metadata.  This can improve performance of random writes, because less metadata must be written.  In practice, at worst about 100 blocks (of recordsize bytes each) of user data can be lost if a single on-disk block is  corrupt.  The exact behavior of which metadata blocks are stored redundantly may change in future releases.  The default value is all.'},
                                      'relatime': {'short_desc': 'Relative access time on read', 'long_desc': 'Controls  the  manner  in which the access time is updated when atime=on is set.  Turning this property on causes the access time to be updated relative to the modify or change time.  Access time is only updated if the previous access time was earlier than the current modify  or change time or if the existing access time has not been updated within the past 24 hours.'},
                                      'reservation': {'short_desc': 'Reservation', 'long_desc': 'The  minimum  amount  of  space  guaranteed to a dataset and its descendents. When the amount of space used is below this value, the dataset is treated as if it were taking up the amount of space specified by its reservation. Reservations are accounted for in the parent datasets space used, and count against the parent datasets quotas and reservations.'},
                                      'secondarycache': {'short_desc': 'Secondary cache', 'long_desc': 'Controls  what  is  cached  in the secondary cache (L2ARC). If this property is set to all, then both user data and metadata is cached. If this property is set to none, then neither user data nor metadata is cached. If this property is set to metadata, then only metadata is cached.  The default value is all.'},
                                      'setuid': {'short_desc': 'Set UID', 'long_desc': 'Controls whether the set-UID bit is respected for the file system.'},
                                      'logbias': {'short_desc': 'Log bias', 'long_desc': 'Provide a hint to ZFS about handling of synchronous requests in this dataset. If logbias is set to latency (the default), ZFS will use pool log devices (if configured) to handle the requests at low latency. If logbias is set to throughput, ZFS will not use configured pool  log  devices.  ZFS will instead optimize synchronous operations for global pool throughput and efficient use of resources.'},
                                      'sync': {'short_desc': 'Synchronous behaviour', 'long_desc': 'Controls the behavior of synchronous requests (e.g. fsync, O_DSYNC).  standard is the POSIX specified  behavior  of  ensuring  all  synchronous requests  are  written  to stable storage and all devices are flushed to ensure data is not cached by device controllers (this is the default).  always causes every file system transaction to be written and flushed before its system call returns. This has  a  large  performance  penalty.  disabled  disables  synchronous requests. File system transactions are only committed to stable storage periodically. This option will give the highest performance.  However, it is very dangerous as ZFS would be ignoring the synchronous transaction demands of applications such as  databases or NFS.  Administrators should only use this option when the risks are understood.'},
                                      'xattr': {'short_desc': 'Extended attributes', 'long_desc': 'Controls  whether extended attributes are enabled for this file system.'}}

        exposed_zvol_specific_properties = {'volblocksize': {'short_desc': 'Volume block size', 'long_desc': 'The block size of the volume. The blocksize cannot be changed once the volume has been written, so it should be  set  at volume creation time. The default blocksize for volumes is 8 Kbytes. Any power of 2 from 512 bytes to 128 Kbytes is valid.'},
                                            'refreservation': {'short_desc': 'Thin provisioning', 'long_desc': 'The minimum amount of space guaranteed to a dataset, not including its descendents. When the amount of space used  is  below  this  value,  the dataset  is treated as if it were taking up the amount of space specified by refreservation. The refreservation reservation is accounted for in the parent datasets space used, and counts against the parent datasets quotas and reservations. If refreservation is set, a snapshot is only allowed if there is enough free pool space outside of this reservation to accommodate the  current number of "referenced" bytes in the dataset.'},
                                            'snapdev': {'short_desc': 'Snapshot devices', 'long_desc': 'Controls whether the snapshots devices of zvols are hidden or visible.'},
                                            'volsize': {'short_desc': 'Volume size', 'long_desc': 'For volumes, specifies the logical size of the volume'}}
        exposed_ds_specific_properties = {'mounted': {'short_desc': 'Mount status', 'long_desc': 'Mount status'},
                                          'refreservation': {'short_desc': 'Reference reservation', 'long_desc': 'The minimum amount of space guaranteed to a dataset, not including its descendents. When the amount of space used  is  below  this  value,  the dataset  is treated as if it were taking up the amount of space specified by refreservation. The refreservation reservation is accounted for in the parent datasets space used, and counts against the parent datasets quotas and reservations. If refreservation is set, a snapshot is only allowed if there is enough free pool space outside of this reservation to accommodate the  current number of "referenced" bytes in the dataset.'},
                                          'canmount': {'short_desc': 'Can mount', 'long_desc': 'If  this  property is set to off, the file system cannot be mounted, and is ignored by zfs automount'},
                                          'quota': {'short_desc': 'Space consumption limit', 'long_desc': 'Limits  the  amount  of  space a dataset and its descendents can consume. This property enforces a hard limit on the amount of space used. This includes all space consumed by descendents, including file systems and snapshots. Setting a quota on a descendent of a dataset that already has a quota does not override the ancestors quota, but rather imposes an additional limit.'},
                                          'mountpoint': {'short_desc': 'Mount point', 'long_desc': 'Controls the mount point used for this file system.'},
                                          'recordsize': {'short_desc': 'Suggested block size', 'long_desc': 'Specifies a suggested block size for files in the file system.'},
                                          'refquota': {'short_desc': 'Reference quota', 'long_desc': 'Limits  the  amount  of  space a dataset can consume. This property enforces a hard limit on the amount of space used. This hard limit does not include space used by descendents, including file systems and snapshots.'},
                                          'snapdir': {'short_desc': 'Snapshot directory visibility', 'long_desc': 'Controls  whether  the  .zfs directory is hidden or visible in the root of the file system'}}

        exposed_properties = {}
        for prop_name in exposed_ds_zvol_properties:
            if prop_name in properties.keys():
                exposed_properties[prop_name] = exposed_ds_zvol_properties[prop_name]
                exposed_properties[prop_name]['value'] = properties[prop_name]['value']
        if is_zvol:
            for prop_name in exposed_zvol_specific_properties:
                if prop_name in properties.keys():
                    exposed_properties[prop_name] = exposed_zvol_specific_properties[prop_name]
                    exposed_properties[prop_name]['value'] = properties[prop_name]['value']
        else:
            for prop_name in exposed_ds_specific_properties:
                if prop_name in properties.keys():
                    exposed_properties[prop_name] = exposed_ds_specific_properties[prop_name]
                    exposed_properties[prop_name]['value'] = properties[prop_name]['value']
        exposed_properties_order = ['type', 'creation', 'volsize', 'volblocksize', 'compression', 'compressratio', 'dedup', 'readonly',  'xattr', 'acltype', 'aclinherit', 'quota', 'refquota', 'canmount', 'mounted', 'mountpoint', 'recordsize', 'reservation', 'refreservation', 'copies',
                                    'usedbydataset', 'usedbychildren', 'snapshot_limit', 'snapdir', 'usedbysnapshots', 'snapdev', 'written', 'logicalused', 'logicalreferenced', 'primarycache', 'secondarycache', 'logbias', 'sync', 'atime', 'relatime', 'checksum', 'redundant_metadata', 'exec', 'setuid', 'devices']
        readonly_properties = ['type', 'creation', 'compressratio', 'mounted', 'usedbysnapshots',
                               'usedbydataset', 'usedbychildren', 'written', 'logicalused', 'logicalreferenced', 'volblocksize']
        modifiable_properties = {}
        for prop_name, prop in exposed_properties.items():
            if prop_name not in readonly_properties:
                modifiable_properties[prop_name] = prop
        for prop_name in exposed_properties.keys():
            if prop_name not in exposed_properties_order:
                raise Exception(
                    '%s not in exposed properties order' % prop_name)
        ordered_exposed_properties = []
        for pname in exposed_properties_order:
            if pname in exposed_properties:
                ordered_exposed_properties.append(
                    {'prop_name': pname, 'prop_dict': exposed_properties[pname]})
        return_dict['ordered_exposed_properties'] = ordered_exposed_properties
        return_dict['exposed_properties'] = exposed_properties
        return_dict['readonly_properties'] = readonly_properties
        return_dict['modifiable_properties'] = modifiable_properties
    except Exception, e:
        return None, 'Error obtaining exposed properties : %s' % str(e)
    else:
        return return_dict, None


"""
Dataset related utility functions
"""


def _get_ds_compononents(line):
    d = None
    try:
        ds_components = line.split()
        if ds_components:
            if ds_components[0].strip():
                d = {}
                d['name'] = ds_components[0]
                d['used'] = ds_components[1]
                d['avail'] = ds_components[2]
                d['refer'] = ds_components[3]
                d['mountpoint'] = ds_components[4]
    except Exception, e:
        return None, str(e)
    else:
        return d, None


def get_datasets_in_pool(pool_name):
    datasets = []
    try:
        cmd = '/sbin/zfs list -r %s -H' % pool_name
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)

        if lines:
            for line in lines:
                if line.strip() == 'no datasets available':
                    break
                td, err = _get_ds_compononents(line)
                if td:
                    if td['name'] != pool_name:
                        prop, err = get_properties(td['name'])
                        if not prop:
                            if err:
                                raise Exception(err)
                            else:
                                raise Exception(
                                    'Error retrieving dataset properties')
                        td['properties'] = prop
                        datasets.append(td)
                else:
                    if err:
                        raise Exception(err)
    except Exception, e:
        return None, 'Error retrieving datasets : %s' % str(e)
    else:
        return datasets, None


def create_dataset(parent, ds_name, properties):
    try:
        if (not ds_name) or (not parent):
            raise Exception('Dataset name or parent not specified')

        cmd = '/sbin/zfs create  '
        if properties:
            for pname, pvalue in properties.items():
                cmd += '-o %s=%s ' % (pname, pvalue)
        cmd += ' %s/%s' % (parent, ds_name)
        # print cmd
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)
        path = '%s/%s' % (parent, ds_name)
        cmd = 'zfs set acltype=posixacl %s' % path
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)
        owner_dict, err = config.get_default_file_dir_owner()
        if err:
            raise Exception(err)
        owner_gid, err = config.get_system_uid_gid(
            owner_dict['group'], 'group')
        if err:
            raise Exception(err)

        os.chown('/%s' % path, -1, owner_gid)
        os.chmod('/%s' % path, stat.S_IWUSR | stat.S_IRUSR | stat.S_IXUSR |
                 stat.S_IRGRP | stat.S_IWGRP | stat.S_IXGRP | stat.S_IROTH | stat.S_IXOTH | stat.S_ISGID)
        os.system('setfacl -d -m g:integralstor:rwx /%s' % path)
        # Not required
        # os.system('setfacl -d -m g::rwx /%s' % path)
        # os.system('setfacl -d -m o::rx /%s' % path)
    except Exception, e:
        return False, 'Error creating dataset : %s ' % str(e)
    else:
        return True, None


def delete_dataset(ds_name, recursive=False, force=False):
    try:
        if (not ds_name):
            raise Exception('Dataset name not specified')

        if recursive and force:
            cmd = '/sbin/zfs destroy -r -f %s' % (ds_name)
        elif recursive:
            cmd = '/sbin/zfs destroy -r %s' % (ds_name)
        elif force:
            cmd = '/sbin/zfs destroy -f %s' % (ds_name)
        else:
            cmd = '/sbin/zfs destroy %s' % (ds_name)
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)
    except Exception, e:
        return False, 'Error deleting dataset : %s ' % str(e)
    else:
        return True, None


def delete_all_datasets(dataset_type='all', recursive=False, force=False):
    error_list = []
    try:
        ds_list, err = get_all_datasets_and_pools(dataset_type)
        if err:
            raise Exception(err)
        pool_list, err = get_all_pool_names()
        if err:
            raise Exception(err)
        datasets = [ds for ds in ds_list if ds not in pool_list]
        for dataset in datasets:
            ret, err = delete_dataset(dataset, recursive, force)
            if err:
                error_list.append(err)
        if error_list:
            raise Exception(str(error_list))
    except Exception, e:
        return False, str(e)
    else:
        return True, None


def delete_all_pools(force=False):
    error_list = []
    try:
        pool_list, err = get_all_pool_names()
        if err:
            raise Exception(err)
        for pool in pool_list:
            ret, err = delete_pool(pool, force)
            if err:
                error_list.append(err)
        if error_list:
            raise Exception(str(error_list))
    except Exception, e:
        return False, str(e)
    else:
        return True, None


def get_children_datasets(ds_name):
    ''' Given a dataset, return a list of all its children datasets '''
    children = []
    try:
        cmd = '/sbin/zfs list -r %s -H -o name' % ds_name
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)

        if lines:
            for line in lines:
                if line.strip() == 'no datasets available':
                    break
                if line.strip() == ds_name:
                    continue
                children.append(line.strip())
        else:
            return None, None
    except Exception, e:
        return None, 'Error retrieving child datasets: %s' % str(e)
    else:
        return children, None


def get_all_datasets_and_pools(dataset_type=None):
    ''' Return a list of all datasets and pools'''
    retlist = []
    try:
        if dataset_type and dataset_type not in ['filesystem', 'snapshot', 'snap', 'volume', 'bookmark', 'all']:
            raise Exception('Invalid parameter')
        if dataset_type:
            cmd = '/sbin/zfs list  -H -o name -t %s' % dataset_type
        else:
            cmd = '/sbin/zfs list  -H -o name '
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)

        if lines:
            for line in lines:
                if line.strip() == 'no datasets available':
                    break
                retlist.append(line.strip())
    except Exception, e:
        return None, 'Error retrieving datasets and pools: %s' % str(e)
    else:
        return retlist, None


"""
Pool related utility functions
"""


def create_pool_data_vdev_list(pool_type, disk_type='rotating', num_raid_disks=None, stripe_width=None):
    """Return a list of the appropriate format based on the pool type using the list of available drives."""
    vdev_list = []
    try:
        #free_disks, err = get_free_disks(disk_type)
        free_disks = []
        fd, err = get_free_disks(disk_type)
        if err:
            raise Exception(err)
        if fd:
            for d in fd:
                if 'capacity' in d:
                    capacity = d['capacity']
                    match = re.search('([0-9\.]*)[ ]*([a-zA-Z]*)', capacity)
                    if match and match.groups():
                        grps = match.groups()
                        if len(grps) > 1:
                            size = float(grps[0].strip())
                            unit = grps[1].strip().lower()
                            if unit == 'gb':
                                if size > 0.5:
                                    free_disks.append(d)
                            elif unit == 'tb':
                                if size > 0.0005:
                                    free_disks.append(d)
                            elif unit == 'mb':
                                if size > 500:
                                    free_disks.append(d)

        if pool_type not in ['mirror', 'raid5', 'raid6', 'raid10', 'raid50', 'raid60']:
            raise Exception('Unsupported pool type specified')
        if pool_type in ['raid5', 'raid6', 'raid50', 'raid60'] and not num_raid_disks:
            raise Exception('Number of disks in the RAID not specified')

        if free_disks:
            num_free_disks = len(free_disks)
            if pool_type == 'mirror':
                if num_free_disks < 2:
                    raise Exception(
                        'Insufficient disks to form a mirrored pool')
                for i in range(2):
                    vdev_list.append(free_disks[i]['id'])
            elif pool_type == 'raid5':
                if num_free_disks < 3:
                    raise Exception('Insufficient disks to form a RAID5 pool')
                for i in range(num_raid_disks):
                    vdev_list.append(free_disks[i]['id'])
            elif pool_type == 'raid6':
                if num_free_disks < 4:
                    raise Exception('Insufficient disks to form a RAID6 pool')
                for i in range(num_raid_disks):
                    vdev_list.append(free_disks[i]['id'])
            elif pool_type in ['raid10', 'raid50', 'raid60']:
                if not stripe_width:
                    raise Exception('Stripe width not specified')
                if pool_type == 'raid10':
                    # Temporarily fake the num_raid_disks for a raid10 pool to
                    # maintain minimum logic below
                    num_raid_disks = 2
                if num_free_disks < (int(stripe_width) * int(num_raid_disks)):
                    raise Exception(
                        'Insufficient disks to form the specified pool type')
                l = []
                for i in range(int(stripe_width) * int(num_raid_disks)):
                    l.append(free_disks[i]['id'])
                    if (i + 1) % int(num_raid_disks) == 0:
                        if l:
                            vdev_list.append(l)
                        l = []
        else:
            raise Exception(
                'There are no free disks of the specified type to form the pool')
    except Exception, e:
        return None, 'Error creating pool data vdev list: %s ' % str(e)
    else:
        return vdev_list, None


def can_expand_pool(pool_name):
    """Check if the pool can be expanded. If it can, return the new pool type also."""
    try:
        if not pool_name:
            raise Exception('Pool name not specified')

        pool, err = get_pool(pool_name)
        if err:
            raise Exception(err)

        free_disks, err = get_free_disks()
        if err:
            raise Exception(err)
        type = pool['config']['pool']['type']
        new_pool_type = type
        # print type
        cmd = None
        if type in ['raid1', 'raid10']:
            # Can expand to a RAID-10
            if len(free_disks) < 2:
                raise Exception(
                    'Insufficient free disks to expand the pool to a RAID-10')
            new_pool_type = 'RAID-10'
        elif type in ['raid5', 'raid6', 'raid50', 'raid60']:
            # Can expand to a RAID-50
            num_raid_disks = len(
                pool['config']['pool']['root']['children'][0]['children'])
            if len(free_disks) < num_raid_disks:
                raise Exception('Insufficient free disks to expand the pool. Need %d disk(s). Have %d disk(s) available.' % (
                    num_raid_disks, len(free_disks)))
            if type in ['raid5', 'raid50']:
                new_pool_type = 'RAID-50'
            elif type in ['raid6', 'raid60']:
                new_pool_type = 'RAID-60'
        elif type == 'raid0':
            # Expand to raid0 itself with more drives
            if len(free_disks) < 1:
                raise Exception('Insufficient free disks to expand the pool')
            new_pool_type = 'RAID-1'
    except Exception, e:
        # print str(e)
        return (False, None), 'Error checking for pool expansion possibility: %s ' % str(e)
    else:
        return (True, new_pool_type), None


def expand_pool(pool_name):
    try:
        if not pool_name:
            raise Exception('Pool name not specified')

        pool, err = get_pool(pool_name)
        if err:
            raise Exception(err)

        free_disks, err = get_free_disks()
        if err:
            raise Exception(err)

        free_disk_ids = []
        for disk in free_disks:
            free_disk_ids.append(disk['id'])

        type = pool['config']['pool']['type']
        # print type
        cmd = None
        if type in ['raid1', 'raid10']:
            # Can expand to a RAID-10
            if len(free_disk_ids) < 2:
                raise Exception(
                    'Insufficient free disks to expand the pool to a RAID-10')
            cmd = '/sbin/zpool add -f  %s mirror %s %s' % (
                pool_name, free_disk_ids[0], free_disk_ids[1])
        elif type in ['raid5', 'raid6', 'raid50', 'raid60']:
            # Can expand to a RAID-50
            num_raid_disks = len(
                pool['config']['pool']['root']['children'][0]['children'])
            if len(free_disk_ids) < num_raid_disks:
                raise Exception('Insufficient free disks to expand the pool. Need %d disk(s). Have %d disk(s) available.' % (
                    num_raid_disks, len(free_disk_ids)))
            if type in ['raid5', 'raid50']:
                cmd = '/sbin/zpool add -f  %s raidz1 ' % pool_name
            elif type in ['raid6', 'raid60']:
                cmd = '/sbin/zpool add -f  %s raidz2 ' % pool_name
            for i in range(num_raid_disks):
                cmd = cmd + " " + free_disk_ids[i]
        elif type == 'raid0':
            # Expand to raid0 itself with more drives
            if len(free_disk_ids) < 1:
                raise Exception('Insufficient free disks to expand the pool')
            cmd = '/sbin/zpool add -f  %s %s' % (pool_name, free_disk_ids[0])
        if not cmd:
            raise Exception(
                'Could not generate the appropriate expansion parameters')
        # print cmd
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)
    except Exception, e:
        # print str(e)
        return False, 'Error expanding pool : %s ' % str(e)
    else:
        return True, None


def update_default_pool_quota(pool_name):
    try:
        QUOTA = 0.85
        quota_str = None
        pd, err = get_pool(pool_name)
        if err:
            raise Exception(err)
        # print pd, err
        # print pd['properties'].keys()
        val_str = pd['properties']['available']['value']
        ret = re.match('([0-9.]+)([A-Za-z]*)', val_str)
        # print ret.groups()
        components = ret.groups()
        if ret and components:
            val = float(ret.groups()[0])
            # print val
            if len(components) > 1:
                quota_str = '%.2f%s' % (val * QUOTA, components[1])
            else:
                quota_str = '%.2f' % (val * QUOTA)
        # print 'quota str', quota_str
        # print 'Setting ZFS pool quota..'
        if quota_str:
            ret, err = update_dataset_zvol_property(
                pool_name, 'quota', quota_str)
            # print 'setting pool quota ', ret, err
            if err:
                raise Exception(err)
    except Exception, e:
        return False, 'Error setting default pool quota: %s ' % str(e)
    else:
        return True, None


def create_pool(pool_name, type, data_vdev_list, log_vdev=None, dedup=False):
    try:
        if not pool_name:
            raise Exception('Pool name not specified')

        if not type:
            raise Exception('Pool type not specified')

        if not data_vdev_list:
            raise Exception('Pool vdevs not specified')

        free_disks, err = get_free_disks()
        if err:
            raise Exception(err)
        free_disk_ids = []
        for disk in free_disks:
            free_disk_ids.append(disk['id'])
        # print 'free disks', free_disks
        # print 'vdevlist', data_vdev_list

        if type == 'mirror':
            if len(data_vdev_list) != 2:
                raise Exception('Only 2 VDEVs supported for mirrored pools ')
            if (data_vdev_list[0] not in free_disk_ids) or (data_vdev_list[1] not in free_disk_ids):
                raise Exception(
                    'Specified disk already in use in another pool!')
            cmd = '/sbin/zpool create -o autoexpand=on -f  %s mirror %s %s' % (
                pool_name, data_vdev_list[0], data_vdev_list[1])
        elif type == 'raid5':
            if len(data_vdev_list) < 3:
                raise Exception(
                    'Need a minimum of 3 VDEVs for a RAID-5 pools ')
            cmd = '/sbin/zpool create -o autoexpand=on  -f  %s raidz1 ' % (
                pool_name)
            for vdev in data_vdev_list:
                if vdev not in free_disk_ids:
                    raise Exception(
                        'Specified disk already in use in another pool!')
                cmd += ' %s ' % vdev
        elif type == 'raid6':
            if len(data_vdev_list) < 4:
                raise Exception(
                    'Need a minimum of 4 VDEVs for a RAID-6 pool! ')
            cmd = '/sbin/zpool create -o autoexpand=on  -f  %s raidz2 ' % (
                pool_name)
            for vdev in data_vdev_list:
                if vdev not in free_disk_ids:
                    raise Exception(
                        'Specified disk already in use in another pool!')
                cmd += ' %s ' % vdev
        elif type in ['raid10', 'raid50', 'raid60']:
            cmd = 'zpool create -f %s ' % (pool_name)
            for vdev in data_vdev_list:
                if (not isinstance(vdev, types.ListType)):
                    raise Exception('Invalid VDEV specification specified')
                for v in vdev:
                    if v not in free_disk_ids:
                        raise Exception(
                            'Specified disk already in use in another pool!')
                if type == 'raid10':
                    cmd += ' mirror %s' % (' '.join(vdev))
                elif type == 'raid50':
                    cmd += ' raidz1 %s' % (' '.join(vdev))
                elif type == 'raid60':
                    cmd += ' raidz2 %s' % (' '.join(vdev))
        # print cmd
        if log_vdev:
            cmd += ' log %s' % log_vdev

        if dedup:
            cmd += ' -O dedup=on '
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception('Error creating the pool : %s' % err)

        ret, err = update_dataset_zvol_property(pool_name, 'xattr', 'sa')
        if err:
            raise Exception(
                'Error setting extended attributes for the pool : %s' % err)

        ret, err = update_dataset_zvol_property(pool_name, 'relatime', 'on')
        if err:
            raise Exception(
                'Error setting extended attributes for the pool : %s' % err)

        ret, err = update_dataset_zvol_property(
            pool_name, 'acltype', 'posixacl')
        if err:
            raise Exception('Error turning on ACLs for the pool : %s' % err)

        ret, err = update_default_pool_quota(pool_name)
        if err:
            raise Exception(err)

    except Exception, e:
        return False, 'Error creating pool : %s ' % str(e)
    else:
        return True, None


def delete_pool(pool_name, force=False):
    try:
        if (not pool_name):
            raise Exception('Pool name not specified')

        if force:
            cmd = '/sbin/zpool destroy -f %s' % (pool_name)
        else:
            cmd = '/sbin/zpool destroy %s' % (pool_name)
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)
        db_path, err = config.get_db_path()
        if db_path:
            query = 'delete from pool_usage_stats where pool_name="%s"' % pool_name
            # print query
            ret, err = db.execute_iud(db_path, [[query]])
            # Dont error out on this call. Just best effort.
            # print ret, err
    except Exception, e:
        return False, 'Error deleting pool : %s ' % str(e)
    else:
        return True, None


def clear_pool(pool_name):
    try:
        if (not pool_name):
            raise Exception('Pool name not specified')

        cmd = '/sbin/zpool clear %s' % (pool_name)
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)
    except Exception, e:
        return False, 'Clearing ZFS pool errors unsuccessful: %s ' % str(e)
    else:
        return True, None


def scrub_pool(pool_name):
    try:
        if (not pool_name):
            raise Exception('Pool name not specified')

        cmd = '/sbin/zpool scrub %s' % (pool_name)
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)
    except Exception, e:
        return False, 'Error initiating pool scrub : %s ' % str(e)
    else:
        return True, None


def delete_pool_vdev(pool_name, vdev):
    try:
        if (not pool_name) or (not vdev):
            raise Exception('Pool name or vdev not specified')

        cmd = '/sbin/zpool remove %s %s' % (pool_name, vdev)
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)
    except Exception, e:
        return False, 'Error removing pool vdev : %s ' % str(e)
    else:
        return True, None


def update_pool_log_vdev(pool_name, vdev):
    try:
        if (not pool_name) or (not vdev):
            raise Exception('Pool name or vdev not specified')

        cmd = '/sbin/zpool add -f %s log %s' % (pool_name, vdev)
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)
    except Exception, e:
        return False, 'Error updating pool log vdev: %s ' % str(e)
    else:
        return True, None


def update_pool_cache_vdev(pool_name, vdev):
    try:
        if (not pool_name) or (not vdev):
            raise Exception('Pool name or vdev not specified')

        cmd = '/sbin/zpool add -f %s cache %s' % (pool_name, vdev)
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)
    except Exception, e:
        return False, 'Error updating pool cache vdev : %s ' % str(e)
    else:
        return True, None


def get_all_pool_names():
    """Return a list names of all pools."""
    retlist = []
    try:
        cmd = '/sbin/zpool list -H -o name'
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)

        if lines:
            for line in lines:
                retlist.append(line.strip())
    except Exception, e:
        return None, 'Error retrieving ZFS pools: %s' % str(e)
    else:
        return retlist, None


def _get_size_components(st):
    d = {}
    try:
        res = re.search('[\s]*([0-9\.]*)[\s]*([a-zA-Z%]*)', st)
        if res:
            grps = res.groups()
            if grps:
                if grps[0]:
                    d['value'] = float(grps[0])
                else:
                    d['value'] = '-'
                if grps[1]:
                    d['unit'] = grps[1]
                else:
                    d['unit'] = ''
    except Exception, e:
        return None, "Error splitting value into numbers and units : %s" % str(e)
    else:
        return d, None


def get_pool_usage(pool_name):
    d = {}
    d['total_space_used_bytes'] = 0
    d['total_space_avail_bytes'] = 0
    d['total_space_bytes'] = 0
    try:
        props, err = get_properties(pool_name)
        if err:
            raise Exception(err)
        if 'used' in props:
            td, err = _get_size_components(props['used']['value'])
            if err:
                raise Exception(err)
            d['used_by_data'] = td
        if 'available' in props:
            td, err = _get_size_components(props['available']['value'])
            if err:
                raise Exception(err)
            d['available_for_data'] = td

        if d['available_for_data'] and d['used_by_data']:
            avail_val = '%s %s' % (
                d['available_for_data']['value'], d['available_for_data']['unit'])
            used_val = '%s %s' % (
                d['used_by_data']['value'], d['used_by_data']['unit'])

            total_size = 0
            total_bytes = 0
            bytes_list, err = filesize.to_bytes([used_val, avail_val])
            if err:
                raise Exception('Could not convert to bytes: %s' % err)
            tb_list, err = filesize.to_tb(bytes_list)
            if err:
                raise Exception('Could not convert to TBs: %s' % err)
            if (bytes_list[0] is not None) and (bytes_list[1] is not None):
                total_bytes = bytes_list[0] + bytes_list[1]

                d['total_space_used_bytes'] = bytes_list[0]
                d['total_space_avail_bytes'] = bytes_list[1]
                d['total_space_bytes'] = total_bytes

            total_tb = tb_list[0] + tb_list[1]
            d['total_space_used_tb'] = tb_list[0]
            d['total_space_avail_tb'] = tb_list[1]
            d['total_space_tb'] = total_tb

            total_size, err = filesize.bytes_to_human_readable_size(
                total_bytes)
            if total_size:
                d['total_space'] = total_size
            else:
                d['total_space'] = 'N.A'

        if d['total_space_bytes'] and d['total_space_used_bytes']:
            d['used_percent'] = float('%.2f' % (
                (d['total_space_used_bytes'] * 100) / d['total_space_bytes']))
            d['avail_percent'] = float('%.2f' % (
                (d['total_space_avail_bytes'] * 100) / d['total_space_bytes']))

        if props['dedup']['value'] == 'on':
            d['dedup'] = 'on'
        else:
            d['dedup'] = 'off'
        pool_props, err = get_properties(pool_name, pool_properties=True)
        if err:
            raise Exception(err)
        if 'allocated' in pool_props:
            td, err = _get_size_components(pool_props['allocated']['value'])
            if err:
                raise Exception(err)
            d['allocated'] = td
        if 'capacity' in pool_props:
            td, err = _get_size_components(pool_props['capacity']['value'])
            if err:
                raise Exception(err)
            d['capacity'] = td
        if 'size' in pool_props:
            td, err = _get_size_components(pool_props['size']['value'])
            if err:
                raise Exception(err)
            d['size'] = td
        if 'dedupratio' in pool_props:
            td, err = _get_size_components(pool_props['dedupratio']['value'])
            if err:
                raise Exception(err)
            d['dedupratio'] = td
        if 'fragmentation' in pool_props:
            td, err = _get_size_components(
                pool_props['fragmentation']['value'])
            if err:
                raise Exception(err)
            d['fragmentation'] = td
        if 'autoexpand' in pool_props:
            if pool_props['autoexpand']['value'] == 'on':
                d['autoexpand'] = True
            else:
                d['autoexpand'] = False
        else:
            d['autoexpand'] = False
        '''
    cmd = 'zpool list -H %s'%pool_name
    lines, err = command.get_command_output(cmd)
    if err:
      raise Exception(err)
    if lines:
      components = lines[0].split()    
      td, err = _get_size_components(components[1])
      if err:
        raise Exception(err)
      d['size'] = td
      td, err = _get_size_components(components[2])
      if err:
        raise Exception(err)
      d['used'] = td
      td, err = _get_size_components(components[3])
      if err:
        raise Exception(err)
      d['free'] = td
      td, err = _get_size_components(components[4])
      if err:
        raise Exception(err)
      d['used_percent'] = td
    '''
    except Exception, e:
        return None, "Error retrieving ZFS pool usage info : %s" % str(e)
    else:
        return d, None


def get_historical_pool_usage_stats():
    return_dict = {}
    try:
        db_path, err = config.get_db_path()
        if err:
            raise Exception(err)
        start_dt, err = datetime_utils.get_epoch(
            when='midnight', num_previous_days=31)
        if err:
            raise Exception(err)
        query = 'select distinct(pool_name) as pool_name from pool_usage_stats where date > "%d"' % start_dt
        # print 'query ', query
        results, err = db.get_multiple_rows(db_path, query)
        # print results, err
        if err:
            raise Exception(err)
        # print results
        dates = []
        ud_list = []
        pool_names = []
        if results:
            for result in results:
                pool_names.append(result['pool_name'])
                query = 'select * from pool_usage_stats where pool_name="%s" and date > %d order by date' % (
                    result['pool_name'], start_dt)
                # print query
                usage_results, err = db.get_multiple_rows(db_path, query)
                # print 'usage', usage_results, err
                if err:
                    raise Exception(err)
                pool_ud_list = []
                for usage_result in usage_results:
                    # print usage_result
                    dt_local, err = datetime_utils.convert_from_epoch(
                        usage_result['date'], return_format='str', str_format='%Y-%m-%d', to='local')
                    if err:
                        raise Exception(err)
                    tb, err = filesize.to_tb([usage_result['available_bytes']])
                    # print 'tb is ', tb
                    if err:
                        raise Exception(err)
                    pool_ud_list.append([dt_local, tb[0]])
                ud_list.append(pool_ud_list)
        #return_dict['dates'] = dates
        #return_dict['usage_data'] = ud_list
        return_dict['pool_names'] = pool_names
        return_dict['usage'] = ud_list
    except Exception, e:
        return None, "Error retrieving historical pool usage statistics : %s" % str(e)
    else:
        return return_dict, None


def get_pools():
    pools = []
    try:
        cmd = '/sbin/zpool status'
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)

        pool_lines = []
        processed = False
        pl = []
        if lines:
            for line in lines:
                if not line.strip():
                    continue
                if 'pool:' in line:
                    # New pool encountered so start a new list
                    if pl:
                        pool_lines.append(pl)
                        pl = []
                pl.append(line)
            if pl:
                pool_lines.append(pl)

            if pool_lines:
                for l in pool_lines:
                    d, err = process_pool(l)
                    if not d:
                        errstr = "Error processing a pool : "
                        if err:
                            errstr += err
                        raise Exception(errstr)
                    d1, err = get_properties(d['pool_name'])
                    if not d1:
                        errstr = "Error getting pool properties : "
                        if err:
                            errstr += err
                        raise Exception(errstr)
                    d['properties'] = d1
                    # Now get related datasets in this pool
                    datasets, err = get_datasets_in_pool(d['pool_name'])
                    if not datasets and err:
                        raise Exception(err)
                    else:
                        d['datasets'] = datasets
                    pools.append(d)

            if pools:
                for pool in pools:
                    usage_dict, err = get_pool_usage(pool['pool_name'])
                    if err:
                        raise Exception(err)
                    pool['usage'] = usage_dict

            snapshots, err = get_snapshots()
            if not snapshots and err:
                raise Exception(err)

            if snapshots:
                if pools:
                    for pool in pools:
                        for snapshot in snapshots:
                            if snapshot['dataset'] == pool['pool_name']:
                                if 'snapshots' not in pool:
                                    pool['snapshots'] = []
                                pool['snapshots'].append(snapshot)
                        # Now add the snapshot info for each dataset
                        if 'snapshots' in pool:
                            datasets = pool['datasets']
                            if datasets:
                                for ds in datasets:
                                    for snapshot in snapshots:
                                        if snapshot['dataset'] == ds['name']:
                                            if 'snapshots' not in ds:
                                                ds['snapshots'] = []
                                            ds['snapshots'].append(snapshot)

    except Exception, e:
        return None, "Error processing zfs pool information : %s" % str(e)
    else:
        return pools, None


def get_pool(pool_name):
    d = None
    try:
        pools, err = get_pools()
        if not pools:
            if err:
                raise Exception(err)
            else:
                raise Exception('Error retrieving pool list')
        for pool in pools:
            if pool['pool_name'] == pool_name:
                d = pool
                break
    except Exception, e:
        return None, "Error retrieving ZFS pool info : %s" % str(e)
    else:
        return d, None


def get_free_disks(disk_type=None, spares_are_free_disks=False):
    free_disks = []
    try:
        pools, err = get_pools()
        if not pools:
            if err:
                errstr = "Error getting pools information : "
                errstr += err
                raise Exception(errstr)

        all_disks, err = disks.get_disk_info_status_all(
            rescan=False, type='info')
        if not all_disks:
            errstr = "Error getting disk information : "
            if err:
                errstr += err
            raise Exception(errstr)
        # print all_disks

        disk_id_list = []
        for sn, disk in all_disks.items():
            if 'id' in disk:
                disk_id_list.append(disk['id'])
        # print disk_id_list

        free_disk_ids = []
        used_disks = []
        for pool in pools:
            if 'config' not in pool:
                continue
            if 'cache' in pool['config'] and pool['config']['cache'] and pool['config']['cache']['root']:
                ud, err = get_disks_in_component(
                    pool['config']['cache']['root'])
                if err:
                    raise Exception(err)
                if ud:
                    used_disks.extend(ud)
            if 'logs' in pool['config'] and pool['config']['logs'] and pool['config']['logs']['root']:
                ud, err = get_disks_in_component(
                    pool['config']['logs']['root'])
                if err:
                    raise Exception(err)
                if ud:
                    used_disks.extend(ud)
            if 'pool' in pool['config'] and pool['config']['pool'] and pool['config']['pool']['root']:
                # print pool['config']['pool']['root']
                ud, err = get_disks_in_component(
                    pool['config']['pool']['root'])
                # print ud
                if err:
                    raise Exception(err)
                if ud:
                    used_disks.extend(ud)
            if not spares_are_free_disks:
                # Count all spares regardless of status as used disks
                if 'spares' in pool['config'] and pool['config']['spares']:
                    ud = pool['config']['spares'].keys()
                    if ud:
                        used_disks.extend(ud)
            else:
                # Check if the spares are available before including them as
                # free disks
                if 'spares' in pool['config'] and pool['config']['spares']:
                    for did, status in pool['config']['spares'].items():
                        if status != 'AVAIL':
                            # Spare is being used so could that as a used disk
                            used_disks.append(did)

        for disk_id in disk_id_list:
            if disk_id not in used_disks:
                free_disk_ids.append(disk_id)

        if free_disk_ids:
            for sn, disk in all_disks.items():
                if disk_type:
                    if disk_type == 'rotational' and not disk['rotational']:
                        # Asking for rotational but current disk is flash so
                        # continue
                        continue
                    if disk_type == 'flash' and disk['rotational']:
                        # Asking for flash but current disk is rotational so
                        # continue
                        continue
                if 'os_device' in disk and disk['os_device'] == True:
                    continue
                if disk['id'] in free_disk_ids:
                    free_disks.append(disk)
    except Exception, e:
        return None, "Error getting free disks : %s" % str(e)
    else:
        return free_disks, None


def get_free_disks_for_spares(pool_name):
    """Given a pool name, returns the list of disks available to use as spares."""
    free_disks = []
    try:
        pool, err = get_pool(pool_name)
        if err:
            raise Exception(err)
        fd, err = get_free_disks(spares_are_free_disks=True)
        if err:
            raise Exception(err)
        # Now make sure that we dont return disks that are already a spare in
        # the requested pool
        pool_spares = None
        if 'spares' in pool['config'] and pool['config']['spares']:
            pool_spares = pool['config']['spares'].keys()
            # print 'pool spares - ', pool_spares
            # print fd
        for disk in fd:
            if not pool_spares or disk['id'] not in pool_spares:
                # Not already a spare in this pool but it is ok if it is a
                # spare in some other pool
                free_disks.append(disk)
    except Exception, e:
        return None, "Error getting free disks for spares: %s" % str(e)
    else:
        return free_disks, None


def add_spares_to_pool(pool_name, num_spares=1):
    try:
        free_disks, err = get_free_disks_for_spares(pool_name)
        if err:
            raise Exception(err)
        if len(free_disks) < num_spares:
            raise Exception(
                'Insufficient number of free disks available for spares. %d free disks currently available.' % len(free_disks))
        lines, err = command.get_command_output(
            'zpool set autoreplace=on %s' % pool_name)
        if err:
            raise Exception(err)
        cmd = 'zpool add -f %s spare ' % pool_name
        for i in range(num_spares):
            cmd += "%s " % free_disks[i]['id']
        # print cmd
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)
    except Exception, e:
        return False, "Error adding spare disks to the pool : %s" % str(e)
    else:
        return True, None


def get_pool_spares(pool_name):
    """Return a list of IDs of the spare disks currently assigned to the pool."""
    spares = []
    try:
        pool, err = get_pool(pool_name)
        if 'spares' in pool['config'] and pool['config']['spares']:
            spares = pool['config']['spares'].keys()
    except Exception, e:
        return None, "Error getting spare disks list for the pool : %s" % str(e)
    else:
        return spares, None


def delete_spare_from_pool(pool_name):
    try:
        spares, err = get_pool_spares(pool_name)
        if err:
            raise Exception(err)
        if len(spares) < 1:
            raise Exception('The pool does not have any spare drives')
        cmd = 'zpool remove  %s %s' % (pool_name, spares[0])
        # print cmd
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)
    except Exception, e:
        return False, "Error removing spare disks from the pool : %s" % str(e)
    else:
        return True, None


def export_pool(pool_name):
    try:
        if not pool_name:
            raise Exception('Pool name not specified')
        cmd = 'zpool export %s' % pool_name
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)
    except Exception, e:
        return False, "Error exporting pool : %s" % str(e)
    else:
        return True, None


def get_exported_pool_names(get_destroyed=False):
    """Get names of pools that are available for importing.

    args:       get_destroyed - A boolean when set True fetches only
                                the names of previously destroyed pools
    returns:    names - A list containing names of available pools
    """
    names = []
    try:
        cmd = 'zpool import'
        if get_destroyed == True:
            cmd += " -D"
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)
        if lines:
            for line in lines:
                if 'pool:' in line:
                    n = line.split(':')
                    if n[1]:
                        names.append(n[1].strip())
    except Exception, e:
        return False, "Error fetching exported pool names: %s" % str(e)
    else:
        return names, None


def import_pool(pool_name):
    try:
        names = []
        if not pool_name:
            raise Exception('Pool name not specified')
        exported_pools, err = get_exported_pool_names()
        if err:
            raise Exception(err)
        destroyed_pools, err = get_exported_pool_names(get_destroyed=True)
        if err:
            raise Exception(err)
        [names.append(name) for name in exported_pools if exported_pools]
        [names.append(name) for name in destroyed_pools if destroyed_pools]

        if pool_name not in names:
            raise Exception('No ZFS pool found by the name %s' % pool_name)

        cmd = None
        if pool_name in destroyed_pools:
            cmd = 'zpool import -Dmf %s' % pool_name
        else:
            cmd = 'zpool import -mf %s' % pool_name
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)

    except Exception, e:
        return False, "Error importing pool: %s" % str(e)
    else:
        return True, None


"""
ZVOL related utility functions
"""


def create_zvol(pool, name, properties, size, unit, block_size='64K', thin=False):
    try:
        if (not name) or (not pool):
            raise Exception('Block device volume name or pool not specified')

        cmd = '/sbin/zfs create  '
        if thin:
            cmd += ' -s '
        cmd += '-b %s -V %d%s ' % (block_size, size, unit)
        if properties:
            for pname, pvalue in properties.items():
                cmd += '-o %s=%s ' % (pname, pvalue)
        cmd += ' %s/%s' % (pool, name)
        # print cmd
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)
    except Exception, e:
        return False, 'Error creating dataset : %s ' % str(e)
    else:
        return True, None


def get_all_zvols():

    zvols = []
    try:
        pools, err = get_pools()
        if err:
            raise Exception(err)
        if pools:
            for pool in pools:
                if pool['datasets']:
                    for ds in pool['datasets']:
                        if ds['properties']['type']['value'] == 'volume':
                            d = {}
                            d['name'] = ds['name']
                            d['path'] = '/dev/zvol/%s' % ds['name']
                            zvols.append(d)
    except Exception, e:
        return None, "Error getting block device volumes : %s" % str(e)
    else:
        return zvols, None


"""
Status check functions - mostly used for alerting
"""


def get_all_components_status(pools):

    status_dict = {}
    try:

        if pools:
            for pool in pools:
                status_list = []
                if 'config' not in pool:
                    continue
                if 'cache' in pool['config'] and pool['config']['cache'] and pool['config']['cache']['root']:
                    sl, err = get_children_component_status(
                        pool['config']['cache']['root'])
                    if err:
                        raise Exception(err)
                    if sl:
                        status_list.extend(sl)
                if 'logs' in pool['config'] and pool['config']['logs'] and pool['config']['logs']['root']:
                    sl, err = get_children_component_status(
                        pool['config']['logs']['root'])
                    if err:
                        raise Exception(err)
                    if sl:
                        status_list.extend(sl)
                if 'pool' in pool['config'] and pool['config']['pool'] and pool['config']['pool']['root']:
                    sl, err = get_children_component_status(
                        pool['config']['pool']['root'])
                    if err:
                        raise Exception(err)
                    if sl:
                        status_list.extend(sl)
                status_dict[pool['pool_name']] = status_list

    except Exception, e:
        return None, "Error getting all components status : %s" % str(e)
    else:
        return status_dict, None


def get_children_component_status(component):
    csl = []
    try:
        if 'children' in component and component['children']:
            for kid in component['children']:
                kid_csl, err = get_children_component_status(kid)
                if err:
                    raise Exception('Error getting disks in use : %s' % err)
                if kid_csl:
                    csl.extend(kid_csl)
        if 'status' in component:
            d = {}
            d['name'] = component['name']
            d['type'] = component['type']
            d['status'] = component['status']
            csl.append(d)
    except Exception, e:
        return None, "Error getting children component status: %s" % str(e)
    else:
        return csl, None


"""
Quota related functions
"""


def get_quota(path, ug_name, user=True):
    """Returns the quota for the particular user or group. Set user=True if user else it is a group."""
    quota = None
    try:
        if not ug_name:
            raise Exception('No user or group name passed')
        if not path:
            raise Exception('Path not specified')
        if user:
            quota_type = 'userquota'
        else:
            quota_type = 'groupquota'
        cmd = 'zfs get %s@%s %s' % (quota_type, ug_name, path)
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)
        for line in lines[1:]:
            components = line.split()
            quota = components[2]
    except Exception, e:
        return None, "Error getting quota : %s" % str(e)
    else:
        return quota, None


def get_all_quotas(path):
    """Returns all user and group quotas for the given path."""
    quotas = {}
    try:
        users = {}
        groups = {}
        if not path:
            raise Exception('Path not specified')
        lines = []
        lines1, err = command.get_command_output('zfs userspace %s' % path)
        if err:
            raise Exception(err)
        if lines1:
            for line in lines1[1:]:
                comps = line.split()
                if comps and comps[-1] != 'none':
                    users[comps[-3]] = comps[-1]
        lines2, err = command.get_command_output('zfs groupspace %s' % path)
        if err:
            raise Exception(err)
        if lines2:
            for line in lines2[1:]:
                comps = line.split()
                if comps and comps[-1] != 'none':
                    groups[comps[-3]] = comps[-1]
        quotas['users'] = users
        quotas['groups'] = groups
    except Exception, e:
        return None, "Error getting all quotas : %s" % str(e)
    else:
        return quotas, None


def update_quota(path, ug_name, quota_amt, user=True):
    """Set user=True if user else it is a group. quota_amt should include the units."""
    try:
        if not ug_name:
            raise Exception('No user or group name passed')
        if not path:
            raise Exception('Path not specified')
        if not quota_amt:
            raise Exception('Quota limit not specified')
        if user:
            quota_type = 'userquota'
        else:
            quota_type = 'groupquota'
        cmd = 'zfs set %s@%s=%s %s' % (quota_type, ug_name, quota_amt, path)
        lines, err = command.get_command_output(cmd)
        if err:
            raise Exception(err)

    except Exception, e:
        return False, "Error updating quota : %s" % str(e)
    else:
        return True, None


def main():
    #d, err = get_pools()
    #d, err = get_snapshot_schedule('p1')
    #d, err = get_snapshot_schedule('p1')
    #d, err = get_latest_snapshot('rr')
    # print execute_remote_replication()
    #d,err = schedule_remote_replication('rr','192.168.1.208','root','rr')
    #d, err = get_all_components_status()
    #d, err = get_all_zvols()
    #d, err = get_pool('p1')
    #d, err = get_pool_usage('zpool')
    #d, err = get_quota('p1', 'ram', False)
    #d, err = update_quota('p1', 'ram', 'none')
    # print d, err
    #d, err = get_all_quotas('p1')
    # print d, err
    #d, err = update_quota('p1', 'ram', '1GB')
    # print d, err
    #d, err = get_all_quotas('p1')
    # print d, err
    #d, err = update_quota('p1', 'ram', '2MB')
    # print d, err
    #d, err = get_all_quotas('p1')
    # print d, err
    #d, err = add_spares_to_pool('p1', 1)
    #d, err = get_pool_spares('p1')
    #d, err = delete_spare_from_pool('p1')
    #d, err = expand_pool('p1')
    #disks, err = get_disks_in_component(d['config']['pool']['root'])
    #d, err = get_properties('pool1')
    #d, err = get_datasets_in_pool('pool1')
    # print create_pool('test_pool', 'mirror' , ['ata-ST1000DM003-1ER162_W4Y1HK70', 'ata-ST1000DM003-1ER162_W4Y1H2CG'])
    #d, err = get_free_disks()
    #d, err = create_pool_data_vdev_list('raid10', 2)
    # if err:
    #  print err
    # else:
    #d, err = update_dataset_zvol_property('tank', 'xattr', 'sa')
    # pp.pprint(disks)
    '''
    #print create_pool('test_pool', 'mirror' , ['ata-ST1000DM003-1ER162_W4Y1HK70', 'ata-ST1000DM003-1ER162_W4Y1H2CG'])
    #print delete_pool('test_pool')
    #d, err = get_free_disks()
    #if err:
    #  print err
    #else
    pp = pprint.PrettyPrinter(indent=4)
    #pp.pprint(d)
    pp.pprint(disks)
  '''
    # print get_pools()
    # print get_all_zvols()
    # print get_datasets_in_pool('tank')
    # print get_all_zvols()
    pp = pprint.PrettyPrinter(indent=4)
    print get_all_pool_names()
    print get_all_datasets_and_pools(dataset_type='filesystem')
    d, err = get_snapshots('tank1/ds1')
    pp.pprint(d)
    pp.pprint(err)
    # print delete_all_datasets(dataset_type='filesystem', recursive=True, force=True)
    # print delete_all_datasets(dataset_type='volume')
    # print delete_all_datasets(dataset_type='snap')
    # print delete_all_pools(force=True)
    # print to_bytes(['1024 K', '1024 M', '10240 KB', '1024 MB'])
    # print get_pool_usage('tank')
    # print delete_all_snapshots('tank/ds2')
    pass


if __name__ == '__main__':
    main()

# vim: tabstop=8 softtabstop=0 expandtab ai shiftwidth=4 smarttab
